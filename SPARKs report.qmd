---
title: "Fire deficit product"
format: docx
editor: source
author: Lora Murphy
date: today
output-file: "Fire deficit product"
---

```{r setup, include=FALSE}
library(terra)
library(sf)
library(kableExtra)
library(ggplot2)
library(exactextractr)
library(tidyverse)
library(tidyterra)
library(patchwork)
library(gpkg)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(messages = FALSE)
knitr::opts_chunk$set(warnings = FALSE)
knitr::opts_chunk$set(results = 'asis')
#\options(knitr.table.format = "latex")
terraOptions(tempdir="D:/workspace/Lora/temp") # Terminator
#terraOptions(tempdir="D:/Lora/temp") # Groot
```

# SPARKs - based on either watershed, county, or L4 ecoregion

We want summary statistics on fire deficit for SPARK watershed regions. To produce these statistics, I used rasterized values of fire deficit where possible, in frequent fire areas. Fire deficit calculations are based on total fires, *including prescribed burns*.

Areas in less-frequent fire areas (FRI > 40) received the value of the ecoregion-level deficit in each pixel. There are different ecoregion-level deficit values for forest, grasslands, and shrublands (see later in this document for details) and the appropriate value was used for each vegetation type. Stats are calculated by aggregating over all the raster pixels in a SPARK, which has the effect of weighting means by area when including the ecoregion-level values.

```{r regions-prep}
#| eval: true

#-----------------------------------------------------------------------------#
# PLEASE NOTE:
# These chunks are now out of order overall. SPARK stuff must be run after the
# raster production later in this document (all marked to eval: false). This
# is rearranged so I can show the latest results first.
#-----------------------------------------------------------------------------#
fri <- terra::rast("../temp_rasters/all_fri_vals.tif")
veg <- terra::rast("../temp_rasters/combined_veg_area.tif")
wui <- terra::rast("../temp_rasters/wui.tif")

#----- Load ecoregions with long-term FRI ------------------------------------#
long_forst_def <- terra::rast("../temp_rasters/eco_forest_def.tif")
long_grass_def <- terra::rast("../temp_rasters/eco_grass_def.tif")
long_shrub_def <- terra::rast("../temp_rasters/eco_shrub_def.tif")

short_def <- terra::rast("../western_us_fire_deficit.tif")


ha_to_acres <- 2.47105

source("functions.R")
```


```{r spark-watersheds-regions}
#| eval: true

#----- Load SPARK watersheds -------------------------------------------------#
spark <- terra::vect("../../Spark counties shapefile/spark_watersheds.shp")
spark <- terra::project(spark, fri)

spark <- spark[-grep("AK -", spark$SPARK)]
spark <- spark[-grep("BC -", spark$SPARK)]

all_sparks <- unique(spark$SPARK)

summary_table_dat <- NULL

for (spark_name in all_sparks) {
  
  one_spark <- spark[spark$SPARK == spark_name]
  summary_dat <- do_a_region(spark_name, one_spark)
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK stats all spark based on watershed.csv", row.names=F)
rm(all_sparks, summary_dat, summary_table_dat)

# Repeat for each watershed within
summary_table_dat <- NULL

for (i in 1:length(spark)) {
  
  summary_dat <- do_a_region(spark$name[i], spark[i])
  summary_dat$SPARK = spark$SPARK[i]
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK watersheds.csv", row.names=F)

rm(spark, summary_dat, summary_table_dat)
```

```{r spark-counties-regions}
#| eval: true

#----- Load SPARK counties ---------------------------------------------------#
spark_co <- terra::vect("../../Spark counties shapefile/spark_counties.shp")
spark_co <- terra::project(spark_co, fri)

spark_co <- spark_co[-grep("AK -", spark_co$SPARK)]
spark_co <- spark_co[!is.na(spark_co$SPARK),]

all_sparks <- unique(spark_co$SPARK)

summary_table_dat <- NULL

for (spark_name in all_sparks) {
  
  one_spark <- spark_co[spark_co$SPARK == spark_name]
  summary_dat <- do_a_region(spark_name, one_spark)
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK stats all spark based on county.csv", row.names=F)
rm(all_sparks, summary_dat, summary_table_dat)

# Repeat for each county within
summary_table_dat <- NULL

for (i in 1:length(spark_co)) {
  
  summary_dat <- do_a_region(spark_co$NAME[i], spark_co[i])
  summary_dat$SPARK = spark_co$SPARK[i]
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK counties.csv", row.names=F)

rm(spark_co, summary_dat, summary_table_dat)
```

```{r spark-ecoregions-regions}
#| eval: true

#----- Load L4 ecoregions for SPARKs -----------------------------------------#
spark_l4 <- terra::vect("../../Spark counties shapefile/spark_l4_ecoregions.shp")
spark_l4 <- terra::project(spark_l4, fri)

summary_table_dat <- NULL

all_eco <- unique(spark_l4$US_L4NAME)
for (spark_name in all_eco) {
  
  one_spark <- spark_l4[spark_l4$US_L4NAME == spark_name]
  summary_dat <- do_a_region(spark_name, one_spark)
  
  if (class(summary_dat$sd_deficit) == "list") stop("here")
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK ecoregions.csv", row.names=F)

rm(spark_co, all_sparks)
```





```{r}
#| eval: false

summary_dat <- read.csv("SPARK stats.csv")
#----- Prep and display table ------------------------------------------------#

summary_dat$meansd <- paste(round(summary_dat$mean_deficit, 2), "$\\pm$", 
                            round(summary_dat$sd_deficit, 2))
summary_dat$mean_deficit <- NULL
summary_dat$sd_deficit <- NULL

summary_dat$Total_area <- prettyNum(round(summary_dat$Total_area, 0), 
                                    big.mark = ",")

summary_dat$Surplus_area <- prettyNum(round(summary_dat$Surplus_area, 0), 
                                      big.mark = ",")

summary_dat$Deficit_area <- prettyNum(round(summary_dat$Deficit_area, 0), 
                                      big.mark = ",")



# Can't do this pretty table automagically for a Word doc
#header_indexer <- rep(5, length(unique(summary_dat$SPARK)))
#names(header_indexer) <- unique(summary_dat$SPARK)

#summary_dat$SPARK <- NULL
#kable(summary_dat, booktabs = TRUE,
#      col.names = c("Vegetation type", "Total area (ac)",
#                "Area in surplus", "Area in deficit",
#                "Mean deficit"), 
#  digits = 1) %>% 
#  pack_rows(index = header_indexer)

print(knitr::kable(summary_dat, col.names = c("Vegetation type", "Total area (ac)",
                                              "Area in surplus", "Area in deficit",
                                              "SPARK", "Mean deficit"), digits = 1))
#}

```

```{r make-spark-barplot}
#| eval: false

summary_dat <- read.csv("SPARK counties.csv")

#----- Take out WUI ----------------------------------------------------------#
x <- grep("WUI", summary_dat$Vegetation)
summary_dat <- summary_dat[-x,]

#----- Take out forest breakdowns --------------------------------------------#
x <- grep("requent", summary_dat$Vegetation)
summary_dat <- summary_dat[-x,]

graph_dat <- left_join(summary_dat, 
                       # Add a total area across the county
                       summary_dat %>% group_by(Region) %>% 
                         summarize(Total = sum(Surplus_area, Deficit_area))) %>%
  
  # Select out some columns that we need
  select(Region, SPARK, Vegetation, Surplus_area, Deficit_area, Total) %>%
  
  # Get relative areas adding up to 1
  mutate(Surplus_area = Surplus_area / Total,
         Deficit_area = Deficit_area / Total) %>%
  
  # Get rid of total column, don't need it now
  select(-Total) %>%
  
  # Reshape for ggplot
  pivot_longer(cols = c(Surplus_area, Deficit_area), values_to = "Area") %>%
  mutate(Vegetation = if_else(Vegetation == "All forest", 
                              "Forest", Vegetation)) %>%
  mutate(name = gsub("_area", "", name)) %>%
  mutate(Vegetation = paste(Vegetation, name)) %>%
  select(-name)

# Levels and colors: apparently have to do in reverse order
veg_levels = c("Shrubland Surplus" = adjustcolor("burlywood4", alpha.f=0.5), 
               "Shrubland Deficit" = "burlywood4", 
               "Grassland Surplus" = adjustcolor("darkgoldenrod3", alpha.f=0.5),
               "Grassland Deficit" = "darkgoldenrod3", 
               "Forest Surplus" = adjustcolor("darkgreen", alpha.f=0.5), 
               "Forest Deficit" = "darkgreen")

# Factors for better grouping
#graph_dat$Region <- as.factor(graph_dat$Region)
graph_dat$Vegetation <- factor(graph_dat$Vegetation, levels = names(veg_levels))

graphs_list = list()
ggplot(data = graph_dat, aes(y = Region, x = Area, fill = Vegetation)) +
  geom_bar(stat='identity') + #, orientation = 'y') +
  scale_fill_manual(values = veg_levels, name="") +
  labs(x = "Relative area", y = "County", 
       title = "Breakdown of county area by vegetation type and deficit status") +
  theme_minimal() +
  facet_wrap(~SPARK, ncol=1, shrink=T, scales="free")



```