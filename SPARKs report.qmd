---
title: "Fire deficit product"
format: docx
editor: source
author: Lora Murphy
date: today
output-file: "Fire deficit product"
---

```{r setup, include=FALSE}
library(terra)
library(sf)
library(kableExtra)
library(ggplot2)
library(exactextractr)
library(tidyverse)
library(tidyterra)
library(patchwork)
library(gpkg)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(messages = FALSE)
knitr::opts_chunk$set(warnings = FALSE)
knitr::opts_chunk$set(results = 'asis')
#\options(knitr.table.format = "latex")
terraOptions(tempdir="D:/workspace/Lora/temp") # Terminator
#terraOptions(tempdir="D:/Lora/temp") # Groot
```

# SPARKs - based on either watershed, county, or L4 ecoregion

We want summary statistics on fire deficit for SPARK watershed regions. To produce these statistics, I used rasterized values of fire deficit where possible, in frequent fire areas. Fire deficit calculations are based on total fires, *including prescribed burns*.

Areas in less-frequent fire areas (FRI > 40) received the value of the ecoregion-level deficit in each pixel. There are different ecoregion-level deficit values for forest, grasslands, and shrublands (see later in this document for details) and the appropriate value was used for each vegetation type. Stats are calculated by aggregating over all the raster pixels in a SPARK, which has the effect of weighting means by area when including the ecoregion-level values.

```{r regions-prep}
#| eval: true

#-----------------------------------------------------------------------------#
# PLEASE NOTE:
# These chunks are now out of order overall. SPARK stuff must be run after the
# raster production later in this document (all marked to eval: false). This
# is rearranged so I can show the latest results first.
#-----------------------------------------------------------------------------#
fri <- terra::rast("../temp_rasters/all_fri_vals.tif")
veg <- terra::rast("../temp_rasters/combined_veg_area.tif")
wui <- terra::rast("../temp_rasters/wui.tif")

#----- Load ecoregions with long-term FRI ------------------------------------#
long_forst_def <- terra::rast("../temp_rasters/eco_forest_def.tif")
long_grass_def <- terra::rast("../temp_rasters/eco_grass_def.tif")
long_shrub_def <- terra::rast("../temp_rasters/eco_shrub_def.tif")

short_def <- terra::rast("../western_us_fire_deficit.tif")


ha_to_acres <- 2.47105

source("functions.R")
```


```{r spark-watersheds-regions}
#| eval: true

#----- Load SPARK watersheds -------------------------------------------------#
spark <- terra::vect("../../Spark counties shapefile/spark_watersheds.shp")
spark <- terra::project(spark, fri)

spark <- spark[-grep("AK -", spark$SPARK)]
spark <- spark[-grep("BC -", spark$SPARK)]

all_sparks <- unique(spark$SPARK)

summary_table_dat <- NULL

for (spark_name in all_sparks) {
  
  one_spark <- spark[spark$SPARK == spark_name]
  summary_dat <- do_a_region(spark_name, one_spark)
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK stats all spark based on watershed.csv", row.names=F)
rm(all_sparks, summary_dat, summary_table_dat)

# Repeat for each watershed within
summary_table_dat <- NULL

for (i in 1:length(spark)) {
  
  summary_dat <- do_a_region(spark$name[i], spark[i])
  summary_dat$SPARK = spark$SPARK[i]
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK watersheds.csv", row.names=F)

rm(spark, summary_dat, summary_table_dat)
```

```{r spark-counties-regions}
#| eval: true

#----- Load SPARK counties ---------------------------------------------------#
spark_co <- terra::vect("../../Spark counties shapefile/spark_counties.shp")
spark_co <- terra::project(spark_co, fri)

all_sparks <- unique(spark_co$SPARK)
all_sparks <- all_sparks[!is.na(all_sparks)]
all_sparks <- all_sparks[-grep("AK -", all_sparks)]

summary_table_dat <- NULL

for (spark_name in all_sparks) {
  
  one_spark <- spark_co[spark_co$SPARK == spark_name]
  summary_dat <- do_a_region(spark_name, one_spark)
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK stats all spark based on county.csv", row.names=F)
rm(all_sparks, summary_dat, summary_table_dat)

# Repeat for each county within
summary_table_dat <- NULL

for (i in 1:length(spark_co)) {
  
  summary_dat <- do_a_region(spark_co$NAME[i], spark_co[i])
  summary_dat$SPARK = spark_co$SPARK[i]
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK counties.csv", row.names=F)

rm(spark_co, summary_dat, summary_table_dat)
```

```{r spark-ecoregions-regions}
#| eval: true

#----- Load L4 ecoregions for SPARKs -----------------------------------------#
spark_l4 <- terra::vect("../../Spark counties shapefile/spark_l4_ecoregions.shp")
spark_l4 <- terra::project(spark_l4, fri)

summary_table_dat <- NULL

all_eco <- unique(spark_l4$US_L4NAME)
for (spark_name in all_eco) {
  
  one_spark <- spark_co[spark_l4$US_L4NAME == spark_name]
  summary_dat <- do_a_region(spark_name, one_spark)
  summary_table_dat <- rbind(summary_table_dat, summary_dat) 
}

write.csv(summary_table_dat, "SPARK ecoregions.csv", row.names=F)

rm(spark_co, all_sparks)
```



```{r}
#| eval: false

summary_dat <- read.csv("SPARK stats.csv")
#----- Prep and display table ------------------------------------------------#

summary_dat$meansd <- paste(round(summary_dat$mean_deficit, 2), "$\\pm$", 
                            round(summary_dat$sd_deficit, 2))
summary_dat$mean_deficit <- NULL
summary_dat$sd_deficit <- NULL

summary_dat$Total_area <- prettyNum(round(summary_dat$Total_area, 0), 
                                    big.mark = ",")

summary_dat$Surplus_area <- prettyNum(round(summary_dat$Surplus_area, 0), 
                                      big.mark = ",")

summary_dat$Deficit_area <- prettyNum(round(summary_dat$Deficit_area, 0), 
                                      big.mark = ",")



# Can't do this pretty table automagically for a Word doc
#header_indexer <- rep(5, length(unique(summary_dat$SPARK)))
#names(header_indexer) <- unique(summary_dat$SPARK)

#summary_dat$SPARK <- NULL
#kable(summary_dat, booktabs = TRUE,
#      col.names = c("Vegetation type", "Total area (ac)",
#                "Area in surplus", "Area in deficit",
#                "Mean deficit"), 
#  digits = 1) %>% 
#  pack_rows(index = header_indexer)

print(knitr::kable(summary_dat, col.names = c("Vegetation type", "Total area (ac)",
                                              "Area in surplus", "Area in deficit",
                                              "SPARK", "Mean deficit"), digits = 1))
#}

```

```{r make-spark-barplot}
#| eval: false

summary_dat <- read.csv("SPARK stats.csv")

#----- Take out WUI ----------------------------------------------------------#
x <- grep("WUI", summary_dat$Forest)
summary_dat <- summary_dat[-x,]


#one <- summary_dat[summary_dat$Region == 'WA - Klickitat County',]
summary_dat$Region <- as.factor(summary_dat$Region)
ggplot(data = summary_dat, aes(y = Region, x = Total_area, fill = Forest)) +
  geom_bar(stat='identity', orientation = 'y')


#----- A nice graph showing all the regions ----------------------------------#
summary_dat$def_lower <- summary_dat$mean_deficit - summary_dat$sd_deficit
summary_dat$def_upper <- summary_dat$mean_deficit + summary_dat$sd_deficit

# Make short display names for SPARKs
summary_dat$SPARK <- NA
x <- which(summary_dat$Region == "AZ - Coconino County-Flagstaff Region")
summary_dat$SPARK[x] <- "AZ Flagstaff"
x <- which(summary_dat$Region == "CA - Santa Barbara County")
summary_dat$SPARK[x] <- "CA Sta. Barbara"
x <- which(summary_dat$Region == "CA - Tahoe Sierra SPARK")
summary_dat$SPARK[x] <- "CA Tahoe"
x <- which(summary_dat$Region == "CO - Gunnison County and Watersheds")
summary_dat$SPARK[x] <- "CO Gunnison Co"
x <- which(summary_dat$Region == "WA - Klickitat County")
summary_dat$SPARK[x] <- "WA Klickitat Co"
x <- which(summary_dat$Region == "WY - Upper Colorado Basin, Green River")
summary_dat$SPARK[x] <- "WY Colorado/Green"

#----- A plot with no restriction on Y axis ----------------------------------#
suppressMessages(region_plot <- 
  ggplot(summary_dat, aes(x = SPARK, y = mean_deficit, color = Forest)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_linerange(
    aes(ymin = def_lower, ymax = def_upper),
    position = position_dodge(width = 0.5)#,
    #width = 0.2
  ) +
  geom_hline(yintercept = 0) +
  labs(
    title = "Mean+sd deficit by SPARK (negative values = deficit)",
    x = "",
    y = "Fire deficit (bars show sd)",
    color = NULL
    #caption = "Two shrubs omitted for readability"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom"))
ggsave("figs/spark_summary_unzoomed.png", region_plot, width=8.25)

#----- A plot zoomed in, to show more central points -------------------------#
xmin = -10
xmax = 5
summary_dat$def_lower2 <- pmax(summary_dat$def_lower, xmin)
summary_dat$def_upper2 <- pmin(summary_dat$def_upper, xmax)
suppressMessages(region_plot <- 
  ggplot(summary_dat, aes(x = SPARK, y = mean_deficit, color = Forest)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_linerange(
    aes(ymin = def_lower2, ymax = def_upper2),
    position = position_dodge(width = 0.5)#,
    #width = 0.2
  ) +
  geom_hline(yintercept = 0) +
  ylim(c(xmin, xmax)) +
  labs(
    title = "Zoomed in: mean+sd deficit by SPARK (negative values = deficit)",
    x = "",
    y = "Fire deficit (bars show sd)",
    color = NULL,
    caption = "Zoomed in for readability, some points omitted"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom"))
ggsave("figs/spark_summary.png", region_plot, width=8.25)
```